library(readxl)
library(factoextra)
library(NbClust)
library(cluster)
library(UniversalCVI)
Whitewine <- read_excel("Whitewine_v6.xlsx")
View(Whitewine)
head(Whitewine)
#Check for missing data
sum(is.na(Whitewine)
#Check for missing data
sum(is.na(Whitewine))
sum(is.na(Whitewine))
#Check for missing data
sum(is.na(Whitewine))
#Function to create boxplots for all features
create_boxplots <- function(dataframe) {
for (column in names(dataframe)) {
boxplot(dataframe[[column]], main = column, ylab = "Value")
}
}
#Function to detect and remove outliers
remove_outliers <- function(dataframe) {
# Initialize an empty vector to store outlier indices
outlier_indices <- c()
for (col in names(dataframe)) {
Q1 <- quantile(dataframe[[col]], 0.25)
Q3 <- quantile(dataframe[[col]], 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.3 * IQR
# Find indices of outliers
col_outlier_indices <- which(dataframe[[col]] < lower_bound | dataframe[[col]] > upper_bound)
# Append outlier indices to the vector
outlier_indices <- c(outlier_indices, col_outlier_indices)
}
# Remove duplicate indices
outlier_indices <- unique(outlier_indices)
# Remove outliers from the dataset
cleaned_data <- dataframe[-outlier_indices, ]
return(cleaned_data)
}
cleaned_whitewine <- remove_outliers(Whitewine)
dim(Whitewine)
dim(cleaned_whitewine)
create_boxplots(Whitewine)
create_boxplots(cleaned_whitewine)
cleaned_quality <- cleaned_whitewine$quality
cleaned_quality
for (feature_name in colnames(chemical_properties)) {
boxplot(Whitewine[[feature_name]], main = paste("Before:", feature_name), ylab = feature_name)
boxplot(cleaned_whitewine[[feature_name]], main = paste("After:", feature_name), ylab = feature_name)
}
# Loop to get before and after boxplot for each feature
for (feature_name in colnames(Whitewine)) {
boxplot(Whitewine[[feature_name]], main = paste("Before:", feature_name), ylab = feature_name)
boxplot(cleaned_whitewine[[feature_name]], main = paste("After:", feature_name), ylab = feature_name)
}
cleaned_quality <- cleaned_whitewine$quality
cleaned_quality
#Taking only the first 11 features
cleaned_chemical_properties <- subset(cleaned_whitewine, select = -quality)
length(cleaned_chemical_properties)
length(cleaned_chemical_properties)
#Scaling chemical properties
scaled_cleaned_chemical_properties <- as.data.frame(scale(cleaned_chemical_properties))
head(scaled_cleaned_chemical_properties)
# NB Clust
nb_results <- NbClust(scaled_cleaned_chemical_properties, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans")
print(nb_results)
# Silhouette
fviz_nbclust(scaled_cleaned_chemical_properties, kmeans,method = "silhouette")
# Elbow Method
fviz_nbclust(scaled_cleaned_chemical_properties, kmeans,method = "wss")
# Gap Stat
fviz_nbclust(scaled_cleaned_chemical_properties, kmeans, method = "gap_stat")
optimal_k <- 2
set.seed(100)
# Perform k-means clustering with the optimal k
kmeans_result <- kmeans(scaled_cleaned_chemical_properties, centers = optimal_k, nstart = 100)
fviz_cluster(kmeans_result, data = scaled_cleaned_chemical_properties, geom = "point", stand = FALSE)
# Display cluster centers
print(kmeans_result$centers)
# Display clustered results
cluster_labels <- kmeans_result$cluster
print(cluster_labels)
library(readxl)
library(factoextra)
library(NbClust)
library(cluster)
library(UniversalCVI)
optimal_k <- 2
set.seed(100)
# Perform k-means clustering
kmeans_result <- kmeans(scaled_cleaned_chemical_properties, centers = optimal_k, nstart = 100)
fviz_cluster(kmeans_result, data = scaled_cleaned_chemical_properties, geom = "point", stand = FALSE)
# Display cluster centers
print(kmeans_result$centers)
# Display clustered results
cluster_labels <- kmeans_result$cluster
print(cluster_labels)
# Total sum of squares (TSS)
tss <- kmeans_result$tot.withinss + kmeans_result$betweenss
# Between-cluster sum of squares (BSS)
bss <- kmeans_result$betweenss
# Within-cluster sum of squares (WSS)
wss <- kmeans_result$tot.withinss
# Ratio of BSS to TSS
bss_tss_ratio <- bss / tss
# Display evaluation metrics
print(paste("BSS:", bss))
print(paste("WSS:", wss))
print(paste("BSS/TSS Ratio:", bss_tss_ratio))
# Calculate silhouette widths
sil_width <- silhouette(kmeans_result$cluster, dist(scaled_cleaned_chemical_properties))
fviz_silhouette(sil_width, geom = "bar", col = kmeans_result$cluster)
cluster_evaluation(scaled_cleaned_chemical_properties)
cluster_evaluation <- function(data, min_clusters = 2, max_clusters = 10, method = "kmeans") {
# Nb Clust
nb_results <- NbClust(data, distance = "euclidean", min.nc = min_clusters, max.nc = max_clusters, method = method)
print(nb_results)
# Silhouette plot
fviz_nbclust(data, kmeans, method = "silhouette")
# Elbow Method
fviz_nbclust(data, kmeans, method = "wss")
# Gap Statistic
fviz_nbclust(data, kmeans, method = "gap_stat")
}
cluster_evaluation(scaled_cleaned_chemical_properties)
# Silhouette
fviz_nbclust(scaled_cleaned_chemical_properties, kmeans,method = "silhouette")
# Elbow Method
fviz_nbclust(scaled_cleaned_chemical_properties, kmeans,method = "wss")
# Gap Stat
fviz_nbclust(scaled_cleaned_chemical_properties, kmeans, method = "gap_stat")
# Perform Principal Component Analysis (PCA)
pca_result <- prcomp(cleaned_chemical_properties, scale. = TRUE)
summary(pca_result)
fviz_eig(pca_result, addlabels = TRUE)
pca_result$rotation
cumulative_proportion <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))
print(cumulative_proportion)
# Identify the number of principal components explaining >85% of variance
selected_pc <- which(cumulative_proportion > 0.85)[1]
print(selected_pc)
plot(cumulative_proportion[0:11], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(v = 7, col="blue", lty=5)
abline(h = 0.8882769 , col="blue", lty=5)
legend("topleft", legend=c("Cut-off @ PC7"),
col=c("blue"), lty=5, cex=0.6)
# Create transformed dataset
transformed_pca_df <- as.data.frame(pca_result$x[, 1:selected_pc])
head(transformed_pca_df)
nb_results <- NbClust(transformed_pca_df, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans")
print(nb_results)
fviz_nbclust(transformed_pca_df, kmeans,method = "silhouette")
fviz_nbclust(transformed_pca_df, kmeans,method = "wss")
fviz_nbclust(transformed_pca_df, kmeans, method = "gap_stat")
optimal_k <- 3
set.seed(100)
kmeans_result <- kmeans(transformed_pca_df, centers = optimal_k, nstart = 100)
fviz_cluster(kmeans_result, data = transformed_pca_df, geom = "point", stand = FALSE)
optimal_k <- 2
set.seed(100)
kmeans_result <- kmeans(transformed_pca_df, centers = optimal_k, nstart = 100)
fviz_cluster(kmeans_result, data = transformed_pca_df, geom = "point", stand = FALSE)
# Display cluster centers
print(kmeans_result$centers)
print(kmeans_result$centers)
cluster_labels <- kmeans_result$cluster
print(cluster_labels)
tss <- kmeans_result$tot.withinss + kmeans_result$betweenss
bss <- kmeans_result$betweenss
wss <- kmeans_result$tot.withinss
bss_tss_ratio <- bss / tss
print(paste("BSS:", bss))
print(paste("WSS:", wss))
print(paste("BSS/TSS Ratio:", bss_tss_ratio))
sil_width <- silhouette(kmeans_result$cluster, dist(transformed_pca_df))
fviz_silhouette(sil_width, geom = "bar", col = kmeans_result$cluster)
# Calculate the Calinski-Harabasz index
ch_index <- CH.IDX(transformed_pca_df, kmax = 10, kmin = 2, method = "kmeans", nstart = 100)
ch_index
plot(ch_index$k, ch_index$CH, type = "b", xlab = "Number of clusters", ylab = "Calinski-Harabasz index", main = "Calinski-Harabasz Index Plot")
library(readxl)
library(neuralnet)
library(Metrics)
ExchangeUSD <- read_excel("ExchangeUSD.xlsx")
head(ExchangeUSD)
exchange_rates <- ExchangeUSD$'USD/EUR'
head(exchange_rates)
length(exchange_rates)
max_rate <- max(exchange_rates)
min_rate <- min(exchange_rates)
min_rate
max_rate
# Function used to normalize
normalize <- function(x, min_val, max_val) {
return((x - min_val) / (max_val - min_val))
}
# Function used to de-normalize
denormalize <- function(x, min_val, max_val) {
return( x*(max_val - min_val) + min_val )
}
calculate_r_squared <- function(actual_values, predicted_values) {
mean_actual <- mean(actual_values)
TSS <- sum((actual_values - mean_actual)^2)
# Calculate residual sum of squares (RSS)
RSS <- sum((actual_values - predicted_values)^2)
R_squared <- 1 - (RSS / TSS)
return(R_squared)
}
draw_scatterplot <- function(predicted, actual,model_name) {
plot(predicted, actual,
xlab = "Predicted Values", ylab = "Actual Values",
main = model_name)
abline(0, 1, col = "red")  # Add a diagonal line representing perfect prediction
}
# Function to create IO matrix
create_io <- function(data, max_time_window) {
io_matrix_list <- list()
for (i in 1:max_time_window) {
inputs <- matrix(NA, nrow = length(data)- i, ncol = i)
for (j in 1:(length(data)- i)) {
inputs[j,] <- data[j:(j + i - 1)]
}
outputs <- data[(i + 1) : (length(data))]
input_df <- as.data.frame(inputs)
colnames(input_df) <- paste0("input_", 1:i)
output_df <- as.data.frame(outputs)
io_matrix_df <- cbind(input_df, output_df)
io_matrix_list[[i]] <- io_matrix_df
}
return(io_matrix_list)
}
io_list <- create_io(exchange_rates, 4)
head(io_list[[4]])
# Function to train NN models
create_mlp_model <- function(hidden_layers, linear_output, activation, learning_rate, data) {
model <- neuralnet(outputs ~ .,
data = data,
hidden = hidden_layers,
linear.output = linear_output,
act.fct = activation,
learningrate = learning_rate)
return(model)
}
mlp_models <- list(
mlp1 = list(data = io_list[[1]], hidden = c(5), linear.output = TRUE, activation = "logistic", learningrate = 0.01),
mlp2 = list(data = io_list[[1]], hidden = c(10), linear.output = FALSE, activation = "tanh", learningrate = 0.0001),
mlp3 = list(data = io_list[[1]], hidden = c(15), linear.output = TRUE, activation = "logistic", learningrate = 0.01),
mlp4 = list(data = io_list[[2]], hidden = c(5), linear.output = TRUE, activation = "tanh", learningrate = 0.0001),
mlp5 = list(data = io_list[[2]], hidden = c(10), linear.output = FALSE, activation = "logistic", learningrate = 0.001),
mlp6 = list(data = io_list[[2]], hidden = c(20, 10), linear.output = TRUE, activation = "tanh", learningrate = 0.001),
mlp7 = list(data = io_list[[2]], hidden = c(10, 5), linear.output = FALSE, activation = "logistic", learningrate = 0.001),
mlp8 = list(data = io_list[[3]], hidden = c(5), linear.output = FALSE, activation = "logistic", learningrate = 0.001),
mlp9 = list(data = io_list[[3]], hidden = c(5, 8), linear.output = TRUE, activation = "tanh", learningrate = 0.0001),
mlp10 = list(data = io_list[[3]], hidden = c(10, 5), linear.output = FALSE, activation = "logistic", learningrate = 0.0001),
mlp11 = list(data = io_list[[3]], hidden = c(20, 10), linear.output = TRUE, activation = "tanh", learningrate = 0.01),
mlp12 = list(data = io_list[[4]], hidden = c(10), linear.output = FALSE, activation = "logistic", learningrate = 0.0001),
mlp13 = list(data = io_list[[4]], hidden = c(5, 5), linear.output = TRUE, activation = "tanh", learningrate = 0.001),
mlp14 = list(data = io_list[[4]], hidden = c(20, 10), linear.output = FALSE, activation = "logistic", learningrate = 0.00001),
mlp15 = list(data = io_list[[4]], hidden = c(10, 15), linear.output = TRUE, activation = "tanh", learningrate = 0.01)
)
model_results <- list()
for (i in names(mlp_models)) {
config <- mlp_models[[i]]
df <- config$data
train_df <- df[1:400,]
test_df <- df[401:nrow(df),]
normalized_train_df <- as.data.frame(lapply(train_df, normalize, min_val = min_rate, max_val = max_rate))
model <- create_mlp_model(config$hidden, config$linear.output, config$activation, config$learningrate, normalized_train_df)
plot(model)
test_inputs <- subset(test_df, select = -outputs)
normalized_test_inputs <- as.data.frame(lapply(test_inputs, normalize, min_val = min_rate, max_val = max_rate))
predicted_values <- compute(model, normalized_test_inputs)
denormalized_predicted_values <- denormalize(predicted_values$net.result, min_rate, max_rate)
actual_outputs <- test_df$outputs
R_Squared <- calculate_r_squared(actual_outputs, denormalized_predicted_values)
RMSE <- sqrt(mean((denormalized_predicted_values - actual_outputs)^2))
MAE <- mean(abs(denormalized_predicted_values - actual_outputs))
MAPE <- mean(abs((denormalized_predicted_values - actual_outputs) / actual_outputs)) * 100
sMAPE <- 2 * mean(abs(denormalized_predicted_values - actual_outputs) / (abs(denormalized_predicted_values) + abs(actual_outputs))) * 100
model_results[[i]] <- c(rsquared = R_Squared, rmse = RMSE, mae = MAE, mape = MAPE, smape = sMAPE)
if (i == "mlp4" || i == "mlp11") {
draw_scatterplot(denormalized_predicted_values, actual_outputs, model_name = i)
}
}
model_results
library(readxl)
library(neuralnet)
library(Metrics)
ExchangeUSD <- read_excel("ExchangeUSD.xlsx")
head(ExchangeUSD)
# Taking only rates column
exchange_rates <- ExchangeUSD$'USD/EUR'
head(exchange_rates)
length(exchange_rates)
max_rate <- max(exchange_rates)
min_rate <- min(exchange_rates)
# Function used to normalize
normalize <- function(x, min_val, max_val) {
return((x - min_val) / (max_val - min_val))
}
# Function used to de-normalize
denormalize <- function(x, min_val, max_val) {
return( x*(max_val - min_val) + min_val )
}
calculate_r_squared <- function(actual_values, predicted_values) {
mean_actual <- mean(actual_values)
TSS <- sum((actual_values - mean_actual)^2)
# Calculate residual sum of squares (RSS)
RSS <- sum((actual_values - predicted_values)^2)
R_squared <- 1 - (RSS / TSS)
return(R_squared)
}
# Function to draw scatterplots
draw_scatterplot <- function(predicted, actual,model_name) {
plot(predicted, actual,
xlab = "Predicted Values", ylab = "Actual Values",
main = model_name)
abline(0, 1, col = "red")
}
# Function to create IO matrix
create_io <- function(data, max_time_window) {
io_matrix_list <- list()
for (i in 1:max_time_window) {
inputs <- matrix(NA, nrow = length(data)- i, ncol = i)
for (j in 1:(length(data)- i)) {
inputs[j,] <- data[j:(j + i - 1)]
}
outputs <- data[(i + 1) : (length(data))]
input_df <- as.data.frame(inputs)
colnames(input_df) <- paste0("input_", 1:i)
output_df <- as.data.frame(outputs)
io_matrix_df <- cbind(input_df, output_df)
io_matrix_list[[i]] <- io_matrix_df
}
return(io_matrix_list)
}
io_list <- create_io(exchange_rates, 4)
head(io_list[[4]])
# Function to train NN models
create_mlp_model <- function(hidden_layers, linear_output, activation, learning_rate, data) {
model <- neuralnet(outputs ~ .,
data = data,
hidden = hidden_layers,
linear.output = linear_output,
act.fct = activation,
learningrate = learning_rate)
return(model)
}
mlp_models <- list(
mlp1 = list(data = io_list[[1]], hidden = c(5), linear.output = TRUE, activation = "logistic", learningrate = 0.01),
mlp2 = list(data = io_list[[1]], hidden = c(10), linear.output = FALSE, activation = "tanh", learningrate = 0.0001),
mlp3 = list(data = io_list[[1]], hidden = c(15), linear.output = TRUE, activation = "logistic", learningrate = 0.01),
mlp4 = list(data = io_list[[2]], hidden = c(5), linear.output = TRUE, activation = "tanh", learningrate = 0.0001),
mlp5 = list(data = io_list[[2]], hidden = c(10), linear.output = FALSE, activation = "logistic", learningrate = 0.001),
mlp6 = list(data = io_list[[2]], hidden = c(20, 10), linear.output = TRUE, activation = "tanh", learningrate = 0.001),
mlp7 = list(data = io_list[[2]], hidden = c(10, 5), linear.output = FALSE, activation = "logistic", learningrate = 0.001),
mlp8 = list(data = io_list[[3]], hidden = c(5), linear.output = FALSE, activation = "logistic", learningrate = 0.001),
mlp9 = list(data = io_list[[3]], hidden = c(5, 8), linear.output = TRUE, activation = "tanh", learningrate = 0.0001),
mlp10 = list(data = io_list[[3]], hidden = c(10, 5), linear.output = FALSE, activation = "logistic", learningrate = 0.0001),
mlp11 = list(data = io_list[[3]], hidden = c(20, 10), linear.output = TRUE, activation = "tanh", learningrate = 0.01),
mlp12 = list(data = io_list[[4]], hidden = c(10), linear.output = FALSE, activation = "logistic", learningrate = 0.0001),
mlp13 = list(data = io_list[[4]], hidden = c(5, 5), linear.output = TRUE, activation = "tanh", learningrate = 0.001),
mlp14 = list(data = io_list[[4]], hidden = c(20, 10), linear.output = FALSE, activation = "logistic", learningrate = 0.00001),
mlp15 = list(data = io_list[[4]], hidden = c(10, 15), linear.output = TRUE, activation = "tanh", learningrate = 0.01)
)
model_results <- list()
for (i in names(mlp_models)) {
config <- mlp_models[[i]]
df <- config$data
train_df <- df[1:400,]
test_df <- df[401:nrow(df),]
normalized_train_df <- as.data.frame(lapply(train_df, normalize, min_val = min_rate, max_val = max_rate))
model <- create_mlp_model(config$hidden, config$linear.output, config$activation, config$learningrate, normalized_train_df)
plot(model)
test_inputs <- subset(test_df, select = -outputs)
normalized_test_inputs <- as.data.frame(lapply(test_inputs, normalize, min_val = min_rate, max_val = max_rate))
predicted_values <- compute(model, normalized_test_inputs)
denormalized_predicted_values <- denormalize(predicted_values$net.result, min_rate, max_rate)
actual_outputs <- test_df$outputs
R_Squared <- calculate_r_squared(actual_outputs, denormalized_predicted_values)
RMSE <- sqrt(mean((denormalized_predicted_values - actual_outputs)^2))
MAE <- mean(abs(denormalized_predicted_values - actual_outputs))
MAPE <- mean(abs((denormalized_predicted_values - actual_outputs) / actual_outputs)) * 100
sMAPE <- 2 * mean(abs(denormalized_predicted_values - actual_outputs) / (abs(denormalized_predicted_values) + abs(actual_outputs))) * 100
model_results[[i]] <- c(rsquared = R_Squared, rmse = RMSE, mae = MAE, mape = MAPE, smape = sMAPE)
if (i == "mlp4" || i == "mlp11") {
draw_scatterplot(denormalized_predicted_values, actual_outputs, model_name = i)
}
}
model_results
for (i in names(mlp_models)) {
config <- mlp_models[[i]]
df <- config$data
train_df <- df[1:400,]
test_df <- df[401:nrow(df),]
normalized_train_df <- as.data.frame(lapply(train_df, normalize, min_val = min_rate, max_val = max_rate))
model <- create_mlp_model(config$hidden, config$linear.output, config$activation, config$learningrate, normalized_train_df)
plot(model)
test_inputs <- subset(test_df, select = -outputs)
normalized_test_inputs <- as.data.frame(lapply(test_inputs, normalize, min_val = min_rate, max_val = max_rate))
predicted_values <- compute(model, normalized_test_inputs)
denormalized_predicted_values <- denormalize(predicted_values$net.result, min_rate, max_rate)
actual_outputs <- test_df$outputs
R_Squared <- calculate_r_squared(actual_outputs, denormalized_predicted_values)
RMSE <- sqrt(mean((denormalized_predicted_values - actual_outputs)^2))
MAE <- mean(abs(denormalized_predicted_values - actual_outputs))
MAPE <- mean(abs((denormalized_predicted_values - actual_outputs) / actual_outputs)) * 100
sMAPE <- 2 * mean(abs(denormalized_predicted_values - actual_outputs) / (abs(denormalized_predicted_values) + abs(actual_outputs))) * 100
model_results[[i]] <- c(rsquared = R_Squared, rmse = RMSE, mae = MAE, mape = MAPE, smape = sMAPE)
if (i == "mlp4" || i == "mlp11") {
draw_scatterplot(denormalized_predicted_values, actual_outputs, model_name = i)
}
}
model_results
for (i in names(mlp_models)) {
config <- mlp_models[[i]]
df <- config$data
train_df <- df[1:400,]
test_df <- df[401:nrow(df),]
normalized_train_df <- as.data.frame(lapply(train_df, normalize, min_val = min_rate, max_val = max_rate))
model <- create_mlp_model(config$hidden, config$linear.output, config$activation, config$learningrate, normalized_train_df)
plot(model)
test_inputs <- subset(test_df, select = -outputs)
normalized_test_inputs <- as.data.frame(lapply(test_inputs, normalize, min_val = min_rate, max_val = max_rate))
predicted_values <- compute(model, normalized_test_inputs)
denormalized_predicted_values <- denormalize(predicted_values$net.result, min_rate, max_rate)
actual_outputs <- test_df$outputs
R_Squared <- calculate_r_squared(actual_outputs, denormalized_predicted_values)
RMSE <- sqrt(mean((denormalized_predicted_values - actual_outputs)^2))
MAE <- mean(abs(denormalized_predicted_values - actual_outputs))
MAPE <- mean(abs((denormalized_predicted_values - actual_outputs) / actual_outputs)) * 100
sMAPE <- 2 * mean(abs(denormalized_predicted_values - actual_outputs) / (abs(denormalized_predicted_values) + abs(actual_outputs))) * 100
model_results[[i]] <- c(rsquared = R_Squared, rmse = RMSE, mae = MAE, mape = MAPE, smape = sMAPE)
if (i == "mlp4" || i == "mlp11") {
draw_scatterplot(denormalized_predicted_values, actual_outputs, model_name = i)
}
}
for (i in names(mlp_models)) {
config <- mlp_models[[i]]
df <- config$data
train_df <- df[1:400,]
test_df <- df[401:nrow(df),]
normalized_train_df <- as.data.frame(lapply(train_df, normalize, min_val = min_rate, max_val = max_rate))
model <- create_mlp_model(config$hidden, config$linear.output, config$activation, config$learningrate, normalized_train_df)
plot(model)
test_inputs <- subset(test_df, select = -outputs)
normalized_test_inputs <- as.data.frame(lapply(test_inputs, normalize, min_val = min_rate, max_val = max_rate))
predicted_values <- compute(model, normalized_test_inputs)
denormalized_predicted_values <- denormalize(predicted_values$net.result, min_rate, max_rate)
actual_outputs <- test_df$outputs
R_Squared <- calculate_r_squared(actual_outputs, denormalized_predicted_values)
RMSE <- sqrt(mean((denormalized_predicted_values - actual_outputs)^2))
MAE <- mean(abs(denormalized_predicted_values - actual_outputs))
MAPE <- mean(abs((denormalized_predicted_values - actual_outputs) / actual_outputs)) * 100
sMAPE <- 2 * mean(abs(denormalized_predicted_values - actual_outputs) / (abs(denormalized_predicted_values) + abs(actual_outputs))) * 100
model_results[[i]] <- c(rsquared = R_Squared, rmse = RMSE, mae = MAE, mape = MAPE, smape = sMAPE)
}
for (i in names(mlp_models)) {
config <- mlp_models[[i]]
df <- config$data
train_df <- df[1:400,]
test_df <- df[401:nrow(df),]
normalized_train_df <- as.data.frame(lapply(train_df, normalize, min_val = min_rate, max_val = max_rate))
model <- create_mlp_model(config$hidden, config$linear.output, config$activation, config$learningrate, normalized_train_df)
plot(model)
test_inputs <- subset(test_df, select = -outputs)
normalized_test_inputs <- as.data.frame(lapply(test_inputs, normalize, min_val = min_rate, max_val = max_rate))
predicted_values <- compute(model, normalized_test_inputs)
denormalized_predicted_values <- denormalize(predicted_values$net.result, min_rate, max_rate)
actual_outputs <- test_df$outputs
R_Squared <- calculate_r_squared(actual_outputs, denormalized_predicted_values)
RMSE <- sqrt(mean((denormalized_predicted_values - actual_outputs)^2))
MAE <- mean(abs(denormalized_predicted_values - actual_outputs))
MAPE <- mean(abs((denormalized_predicted_values - actual_outputs) / actual_outputs)) * 100
sMAPE <- 2 * mean(abs(denormalized_predicted_values - actual_outputs) / (abs(denormalized_predicted_values) + abs(actual_outputs))) * 100
model_results[[i]] <- c(rsquared = R_Squared, rmse = RMSE, mae = MAE, mape = MAPE, smape = sMAPE)
}
for (i in names(mlp_models)) {
config <- mlp_models[[i]]
df <- config$data
train_df <- df[1:400,]
test_df <- df[401:nrow(df),]
normalized_train_df <- as.data.frame(lapply(train_df, normalize, min_val = min_rate, max_val = max_rate))
model <- create_mlp_model(config$hidden, config$linear.output, config$activation, config$learningrate, normalized_train_df)
plot(model)
test_inputs <- subset(test_df, select = -outputs)
normalized_test_inputs <- as.data.frame(lapply(test_inputs, normalize, min_val = min_rate, max_val = max_rate))
predicted_values <- compute(model, normalized_test_inputs)
denormalized_predicted_values <- denormalize(predicted_values$net.result, min_rate, max_rate)
actual_outputs <- test_df$outputs
R_Squared <- calculate_r_squared(actual_outputs, denormalized_predicted_values)
RMSE <- sqrt(mean((denormalized_predicted_values - actual_outputs)^2))
MAE <- mean(abs(denormalized_predicted_values - actual_outputs))
MAPE <- mean(abs((denormalized_predicted_values - actual_outputs) / actual_outputs)) * 100
sMAPE <- 2 * mean(abs(denormalized_predicted_values - actual_outputs) / (abs(denormalized_predicted_values) + abs(actual_outputs))) * 100
model_results[[i]] <- c(rsquared = R_Squared, rmse = RMSE, mae = MAE, mape = MAPE, smape = sMAPE)
#if (i == "mlp4" || i == "mlp11") {
#  draw_scatterplot(denormalized_predicted_values, actual_outputs, model_name = i)
#}
}
plot(ch_index$k, ch_index$CH, type = "b", xlab = "Number of clusters", ylab = "Calinski-Harabasz index", main = "Calinski-Harabasz Index Plot")
model_results
